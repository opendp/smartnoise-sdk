{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential, Sigmoid\n",
    "\n",
    "import opacus\n",
    "\n",
    "from ctgan.data_sampler import DataSampler\n",
    "from ctgan.data_transformer import DataTransformer\n",
    "from ctgan.synthesizers import CTGANSynthesizer\n",
    "\n",
    "\n",
    "class Discriminator(Module):\n",
    "    def __init__(self, input_dim, discriminator_dim, loss, pac=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        torch.cuda.manual_seed(0)\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        dim = input_dim * pac\n",
    "        #  print ('now dim is {}'.format(dim))\n",
    "        self.pac = pac\n",
    "        self.pacdim = dim\n",
    "\n",
    "        seq = []\n",
    "        for item in list(discriminator_dim):\n",
    "            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n",
    "            dim = item\n",
    "\n",
    "        seq += [Linear(dim, 1)]\n",
    "        if loss == \"cross_entropy\":\n",
    "            seq += [Sigmoid()]\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
    "        alpha = alpha.repeat(1, pac, real_data.size(1))\n",
    "        alpha = alpha.view(-1, real_data.size(1))\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "        disc_interpolates = self(interpolates)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=disc_interpolates, inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
    "            create_graph=True, retain_graph=True, only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        gradient_penalty = ((\n",
    "            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n",
    "        ) ** 2).mean() * lambda_\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def forward(self, input):\n",
    "        assert input.size()[0] % self.pac == 0\n",
    "        return self.seq(input.view(-1, self.pacdim))\n",
    "\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, i, o):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fc = Linear(i, o)\n",
    "        self.bn = BatchNorm1d(o)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.fc(input)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return torch.cat([out, input], dim=1)\n",
    "\n",
    "\n",
    "class Generator(Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, generator_dim, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        dim = embedding_dim\n",
    "        seq = []\n",
    "        for item in list(generator_dim):\n",
    "            seq += [Residual(dim, item)]\n",
    "            dim += item\n",
    "        seq.append(Linear(dim, data_dim))\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        data = self.seq(input)\n",
    "        return data\n",
    "\n",
    "\n",
    "# custom for calcuate grad_sample for multiple loss.backward()\n",
    "def _custom_create_or_extend_grad_sample(\n",
    "    param: torch.Tensor, grad_sample: torch.Tensor, batch_dim: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a 'grad_sample' attribute in the given parameter, or accumulate it\n",
    "    if the 'grad_sample' attribute already exists.\n",
    "    This custom code will not work when using optimizer.virtual_step()\n",
    "    \"\"\"\n",
    "\n",
    "    # print (\"now this happen\")\n",
    "\n",
    "    if hasattr(param, \"grad_sample\"):\n",
    "        param.grad_sample = param.grad_sample + grad_sample\n",
    "        # param.grad_sample = torch.cat((param.grad_sample, grad_sample), batch_dim)\n",
    "    else:\n",
    "        param.grad_sample = grad_sample\n",
    "\n",
    "\n",
    "class DPCTGAN(CTGANSynthesizer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim=128,\n",
    "                 generator_dim=(256, 256),\n",
    "                 discriminator_dim=(256, 256),\n",
    "                 generator_lr=2e-4,\n",
    "                 generator_decay=1e-6,\n",
    "                 discriminator_lr=2e-4,\n",
    "                 discriminator_decay=1e-6,\n",
    "                 batch_size=500,\n",
    "                 discriminator_steps=1,\n",
    "                 log_frequency=True,\n",
    "                 verbose=True,\n",
    "                 epochs=300,\n",
    "                 pac=10,\n",
    "                 cuda=True,\n",
    "                 disabled_dp=False,\n",
    "                 target_delta=None,\n",
    "                 sigma=5,\n",
    "                 max_per_sample_grad_norm=1.0,\n",
    "                 epsilon=1,\n",
    "                 loss=\"cross_entropy\",):\n",
    "\n",
    "        assert batch_size % 2 == 0\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._generator_dim = generator_dim\n",
    "        self._discriminator_dim = discriminator_dim\n",
    "\n",
    "        self._generator_lr = generator_lr\n",
    "        self._generator_decay = generator_decay\n",
    "        self._discriminator_lr = discriminator_lr\n",
    "        self._discriminator_decay = discriminator_decay\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._discriminator_steps = discriminator_steps\n",
    "        self._log_frequency = log_frequency\n",
    "        self._verbose = verbose\n",
    "        self._epochs = epochs\n",
    "        self.pac = pac\n",
    "\n",
    "        # opacus parameters\n",
    "        self.sigma = sigma\n",
    "        self.disabled_dp = disabled_dp\n",
    "        self.target_delta = target_delta\n",
    "        self.max_per_sample_grad_norm = max_per_sample_grad_norm\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_list = []\n",
    "        self.alpha_list = []\n",
    "        self.loss_d_list = []\n",
    "        self.loss_g_list = []\n",
    "        self.verbose = verbose\n",
    "        self.loss = loss\n",
    "\n",
    "        if not cuda or not torch.cuda.is_available():\n",
    "            device = 'cpu'\n",
    "        elif isinstance(cuda, str):\n",
    "            device = cuda\n",
    "        else:\n",
    "            device = 'cuda'\n",
    "\n",
    "        self._device = torch.device(device)\n",
    "\n",
    "        self._transformer = None\n",
    "        self._data_sampler = None\n",
    "        self._generator = None\n",
    "\n",
    "        if self.loss != \"cross_entropy\":\n",
    "            # Monkeypatches the _create_or_extend_grad_sample function when calling opacus\n",
    "            opacus.supported_layers_grad_samplers._create_or_extend_grad_sample = (\n",
    "                _custom_create_or_extend_grad_sample\n",
    "            )\n",
    "\n",
    "    def train(self, data, categorical_columns=None, ordinal_columns=None, update_epsilon=None):\n",
    "        if update_epsilon:\n",
    "            self.epsilon = update_epsilon\n",
    "\n",
    "        self._transformer = DataTransformer()\n",
    "        self._transformer.fit(data, discrete_columns=categorical_columns)\n",
    "\n",
    "        train_data = self._transformer.transform(data)\n",
    "\n",
    "        self._data_sampler = DataSampler(\n",
    "            train_data,\n",
    "            self._transformer.output_info_list,\n",
    "            self._log_frequency)\n",
    "\n",
    "        data_dim = self._transformer.output_dimensions\n",
    "\n",
    "        self._generator = Generator(\n",
    "            self._embedding_dim + self._data_sampler.dim_cond_vec(),\n",
    "            self._generator_dim,\n",
    "            data_dim\n",
    "        ).to(self._device)\n",
    "\n",
    "        discriminator = Discriminator(\n",
    "            data_dim + self._data_sampler.dim_cond_vec(),\n",
    "            self._discriminator_dim,\n",
    "            self.loss,\n",
    "            self.pac\n",
    "        ).to(self._device)\n",
    "\n",
    "        optimizerG = optim.Adam(\n",
    "            self._generator.parameters(),\n",
    "            lr=self._generator_lr,\n",
    "            betas=(0.5, 0.9),\n",
    "            weight_decay=self._generator_decay\n",
    "        )\n",
    "        optimizerD = optim.Adam(\n",
    "            discriminator.parameters(),\n",
    "            lr=self._discriminator_lr,\n",
    "            betas=(0.5, 0.9),\n",
    "            weight_decay=self._discriminator_decay\n",
    "        )\n",
    "\n",
    "        privacy_engine = opacus.PrivacyEngine(\n",
    "            discriminator,\n",
    "            batch_size=self._batch_size,\n",
    "            sample_size=train_data.shape[0],\n",
    "            alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
    "            noise_multiplier=self.sigma,\n",
    "            max_grad_norm=self.max_per_sample_grad_norm,\n",
    "            clip_per_layer=True,\n",
    "        )\n",
    "\n",
    "        if not self.disabled_dp:\n",
    "            privacy_engine.attach(optimizerD)\n",
    "\n",
    "        one = torch.tensor(1, dtype=torch.float).to(self._device)\n",
    "        mone = one * -1\n",
    "\n",
    "        real_label = 1\n",
    "        fake_label = 0\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        assert self._batch_size % 2 == 0\n",
    "        mean = torch.zeros(self._batch_size, self._embedding_dim, device=self._device)\n",
    "        std = mean + 1\n",
    "\n",
    "        steps_per_epoch = max(len(train_data) // self._batch_size, 1)\n",
    "        for i in range(self._epochs):\n",
    "            if not self.disabled_dp:\n",
    "                # if self.loss == 'cross_entropy':\n",
    "                #    autograd_grad_sample.clear_backprops(discriminator)\n",
    "                # else:\n",
    "                for p in discriminator.parameters():\n",
    "                    if hasattr(p, \"grad_sample\"):\n",
    "                        del p.grad_sample\n",
    "\n",
    "                if self.target_delta is None:\n",
    "                    self.target_delta = 1 / train_data.shape[0]\n",
    "\n",
    "                epsilon, best_alpha = optimizerD.privacy_engine.get_privacy_spent(\n",
    "                    self.target_delta\n",
    "                )\n",
    "\n",
    "                self.epsilon_list.append(epsilon)\n",
    "                self.alpha_list.append(best_alpha)\n",
    "            \n",
    "            if not self.disabled_dp:\n",
    "                if self.epsilon < epsilon:\n",
    "                    \n",
    "                    if self._epochs == 1:\n",
    "                        raise ValueError(\"Given epsilon and sigma parameters are too small to\"\n",
    "                        + \" create a private dataset. Try increasing either parameter and rerunning.\")\n",
    "                    else:\n",
    "                        break\n",
    "                    \n",
    "            \n",
    "            for id_ in range(steps_per_epoch):\n",
    "                fakez = torch.normal(mean=mean, std=std)\n",
    "\n",
    "                condvec = self._data_sampler.sample_condvec(self._batch_size)\n",
    "                if condvec is None:\n",
    "                    c1, m1, col, opt = None, None, None, None\n",
    "                    real = self._data_sampler.sample_data(self._batch_size, col, opt)\n",
    "                else:\n",
    "                    c1, m1, col, opt = condvec\n",
    "                    c1 = torch.from_numpy(c1).to(self._device)\n",
    "                    m1 = torch.from_numpy(m1).to(self._device)\n",
    "                    fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                    perm = np.arange(self._batch_size)\n",
    "                    np.random.shuffle(perm)\n",
    "                    real = self._data_sampler.sample_data(\n",
    "                        self._batch_size, col[perm], opt[perm])\n",
    "                    c2 = c1[perm]\n",
    "\n",
    "                fake = self._generator(fakez)\n",
    "                fakeact = self._apply_activate(fake)\n",
    "\n",
    "                real = torch.from_numpy(real.astype(\"float32\")).to(self._device)\n",
    "\n",
    "                if c1 is not None:\n",
    "                    fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "                    real_cat = torch.cat([real, c2], dim=1)\n",
    "                else:\n",
    "                    real_cat = real\n",
    "                    fake_cat = fakeact\n",
    "\n",
    "                optimizerD.zero_grad()\n",
    "\n",
    "                if self.loss == \"cross_entropy\":\n",
    "                    y_fake = discriminator(fake_cat)\n",
    "\n",
    "                    #   print ('y_fake is {}'.format(y_fake))\n",
    "                    label_fake = torch.full(\n",
    "                        (int(self._batch_size / self.pac),),\n",
    "                        fake_label,\n",
    "                        dtype=torch.float,\n",
    "                        device=self._device,\n",
    "                    )\n",
    "\n",
    "                    #    print ('label_fake is {}'.format(label_fake))\n",
    "\n",
    "                    error_d_fake = criterion(y_fake.squeeze(), label_fake)\n",
    "                    error_d_fake.backward()\n",
    "                    optimizerD.step()\n",
    "\n",
    "                    # train with real\n",
    "                    label_true = torch.full(\n",
    "                        (int(self._batch_size / self.pac),),\n",
    "                        real_label,\n",
    "                        dtype=torch.float,\n",
    "                        device=self._device,\n",
    "                    )\n",
    "                    y_real = discriminator(real_cat)\n",
    "                    error_d_real = criterion(y_real.squeeze(), label_true)\n",
    "                    error_d_real.backward()\n",
    "                    optimizerD.step()\n",
    "\n",
    "                    loss_d = error_d_real + error_d_fake\n",
    "\n",
    "                else:\n",
    "\n",
    "                    y_fake = discriminator(fake_cat)\n",
    "                    mean_fake = torch.mean(y_fake)\n",
    "                    mean_fake.backward(one)\n",
    "\n",
    "                    y_real = discriminator(real_cat)\n",
    "                    mean_real = torch.mean(y_real)\n",
    "                    mean_real.backward(mone)\n",
    "\n",
    "                    optimizerD.step()\n",
    "\n",
    "                    loss_d = -(mean_real - mean_fake)\n",
    "\n",
    "                max_grad_norm = []\n",
    "                for p in discriminator.parameters():\n",
    "                    param_norm = p.grad.data.norm(2).item()\n",
    "                    max_grad_norm.append(param_norm)\n",
    "                # pen = calc_gradient_penalty(discriminator, real_cat, fake_cat, self.device)\n",
    "\n",
    "                # pen.backward(retain_graph=True)\n",
    "                # loss_d.backward()\n",
    "                # optimizer_d.step()\n",
    "\n",
    "                fakez = torch.normal(mean=mean, std=std)\n",
    "                condvec = self._data_sampler.sample_condvec(self._batch_size)\n",
    "\n",
    "                if condvec is None:\n",
    "                    c1, m1, col, opt = None, None, None, None\n",
    "                else:\n",
    "                    c1, m1, col, opt = condvec\n",
    "                    c1 = torch.from_numpy(c1).to(self._device)\n",
    "                    m1 = torch.from_numpy(m1).to(self._device)\n",
    "                    fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                fake = self._generator(fakez)\n",
    "                fakeact = self._apply_activate(fake)\n",
    "\n",
    "                if c1 is not None:\n",
    "                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "                else:\n",
    "                    y_fake = discriminator(fakeact)\n",
    "\n",
    "                # if condvec is None:\n",
    "                cross_entropy = 0\n",
    "                # else:\n",
    "                #    cross_entropy = self._cond_loss(fake, c1, m1)\n",
    "\n",
    "                if self.loss == \"cross_entropy\":\n",
    "                    label_g = torch.full(\n",
    "                        (int(self._batch_size / self.pac),),\n",
    "                        real_label,\n",
    "                        dtype=torch.float,\n",
    "                        device=self._device,\n",
    "                    )\n",
    "                    # label_g = torch.full(int(self.batch_size/self.pack,),1,device=self.device)\n",
    "                    loss_g = criterion(y_fake.squeeze(), label_g)\n",
    "                    loss_g = loss_g + cross_entropy\n",
    "                else:\n",
    "                    loss_g = -torch.mean(y_fake) + cross_entropy\n",
    "\n",
    "                optimizerG.zero_grad()\n",
    "                loss_g.backward()\n",
    "                optimizerG.step()\n",
    "\n",
    "                \n",
    "                    # if self.verbose:\n",
    "\n",
    "            # if not self.disabled_dp:\n",
    "            #     if self.epsilon < epsilon:\n",
    "            #         break\n",
    "                \n",
    "                ## ADD RAISE WARNING HERE:\n",
    "                \n",
    "            self.loss_d_list.append(loss_d)\n",
    "            self.loss_g_list.append(loss_g)\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    \"Epoch %d, Loss G: %.4f, Loss D: %.4f\"\n",
    "                    % (i + 1, loss_g.detach().cpu(), loss_d.detach().cpu()),\n",
    "                    flush=True,\n",
    "                )\n",
    "                print(\"epsilon is {e}, alpha is {a}\".format(e=epsilon, a=best_alpha))\n",
    "\n",
    "        return self.loss_d_list, self.loss_g_list, self.epsilon_list, self.alpha_list\n",
    "\n",
    "    def generate(self, n, condition_column=None, condition_value=None):\n",
    "        \"\"\"\n",
    "        TODO: Add condition_column support from CTGAN\n",
    "        \"\"\"\n",
    "        self._generator.eval()\n",
    "\n",
    "        # output_info = self._transformer.output_info\n",
    "        steps = n // self._batch_size + 1\n",
    "        data = []\n",
    "        for i in range(steps):\n",
    "            mean = torch.zeros(self._batch_size, self._embedding_dim)\n",
    "            std = mean + 1\n",
    "            fakez = torch.normal(mean=mean, std=std).to(self._device)\n",
    "\n",
    "            condvec = self._data_sampler.sample_original_condvec(self._batch_size)\n",
    "\n",
    "            if condvec is None:\n",
    "                pass\n",
    "            else:\n",
    "                c1 = condvec\n",
    "                c1 = torch.from_numpy(c1).to(self._device)\n",
    "                fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "            fake = self._generator(fakez)\n",
    "            fakeact = self._apply_activate(fake)\n",
    "            data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        data = data[:n]\n",
    "\n",
    "        return self._transformer.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 10%|█         | 1/10 [00:01<00:16,  1.81s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 20%|██        | 2/10 [00:03<00:14,  1.80s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 30%|███       | 3/10 [00:05<00:12,  1.78s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 40%|████      | 4/10 [00:07<00:10,  1.75s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 5/10 [00:08<00:08,  1.74s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 6/10 [00:10<00:06,  1.73s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 7/10 [00:12<00:05,  1.73s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 8/10 [00:13<00:03,  1.73s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 9/10 [00:15<00:01,  1.72s/it]/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/lucasrosenblatt/opt/miniconda3/envs/test-clone/lib/python3.8/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 outputs from d1 have more than 25000.0 X\n",
      "7 outputs from d2 have more than 25000.0 X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from snsynth.pytorch import PytorchDPSynthesizer\n",
    "\n",
    "# from snsynth.pytorch.nn import DPCTGAN\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "trials = 10\n",
    "\n",
    "size = 50000\n",
    "\n",
    "eps = 1.0 / size\n",
    "\n",
    "\n",
    "\n",
    "X, Y =\"X\", \"Y\"\n",
    "\n",
    "data1 = pd.DataFrame(columns=[\"A\"], data=np.array([[Y] + [X] * size]).T)\n",
    "\n",
    "data2 = pd.DataFrame(columns=[\"A\"], data=np.array([[X] + [Y] * size]).T)\n",
    "\n",
    "outcomes1 = []\n",
    "\n",
    "outcomes2 = []\n",
    "\n",
    "for i in trange(trials):\n",
    "    synth1 = DPCTGAN(epsilon=eps)\n",
    "    synth1.train(data1, categorical_columns=[\"A\"])\n",
    "    sample1 = synth1.generate(size)\n",
    "    success1 = sum(sample1[\"A\"] == X) > size / 2\n",
    "\n",
    "    outcomes1.append(success1)\n",
    "\n",
    "    synth2 = DPCTGAN(epsilon=eps)\n",
    "\n",
    "    synth2.train(data2, categorical_columns=[\"A\"])\n",
    "\n",
    "    sample2 = synth2.generate(size)\n",
    "\n",
    "    success2 = sum(sample2[\"A\"] == X) > size / 2\n",
    "\n",
    "    outcomes2.append(success2)\n",
    "\n",
    "print(f\"{sum(outcomes1)} outputs from d1 have more than {size/2} X\")\n",
    "\n",
    "print(f\"{sum(outcomes2)} outputs from d2 have more than {size/2} X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7a9e8d2ad9440831fd6b3d2d47286ad8a84c16ab070148482e9d67b4332385d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('test-clone': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
