{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing GSD and AIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For GSD support, please install jax: pip install --upgrade  \"jax[cuda11_cudnn82]==0.4.6\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import utils\n",
    "from snsynth import Synthesizer\n",
    "from snsynth.gsd import GSDSynthesizer\n",
    "from snsynth.aim import AIMSynthesizer\n",
    "from load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading downloaded_datasets/adult.csv\n",
      "Memory consumed by adult:4167808\n",
      "Index(['workclass', 'education', 'marital-status', 'occupation',\n",
      "       'relationship', 'race', 'sex', 'native-country', 'earning-class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "adult_path = 'adult.csv'\n",
    "datasets = load_data(['adult'])\n",
    "    \n",
    "adult_df = datasets['adult']['data']\n",
    "\n",
    "target =  datasets['adult']['target']\n",
    "categorical_columns =  datasets['adult']['categorical_columns'].split(',')\n",
    "\n",
    "adult_df = adult_df[categorical_columns]\n",
    "print(adult_df.columns)\n",
    "\n",
    "\n",
    "adult_df_train, adult_df_test = train_test_split(adult_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with 76204800 dimensions\n",
      "9\n",
      "Initial Sigma 73.09534912965321\n",
      "Selected ('col2', 'col4') Size 42 Budget Used 0.06319444444444443\n",
      "Selected ('col4', 'col6') Size 12 Budget Used 0.07013888888888886\n",
      "Selected ('col2', 'col8') Size 14 Budget Used 0.07708333333333331\n",
      "Selected ('col3', 'col6') Size 30 Budget Used 0.08402777777777774\n",
      "Selected ('col3', 'col8') Size 30 Budget Used 0.09097222222222219\n",
      "Selected ('col1', 'col8') Size 32 Budget Used 0.09791666666666662\n",
      "Selected ('col0', 'col3') Size 135 Budget Used 0.10486111111111107\n",
      "Selected ('col5', 'col6') Size 10 Budget Used 0.1118055555555555\n",
      "Selected ('col0', 'col6') Size 18 Budget Used 0.11874999999999994\n",
      "Selected ('col5', 'col8') Size 10 Budget Used 0.1256944444444444\n",
      "Selected ('col2', 'col6') Size 14 Budget Used 0.13263888888888883\n",
      "Selected ('col4', 'col8') Size 12 Budget Used 0.13958333333333328\n",
      "Selected ('col6', 'col8') Size 4 Budget Used 0.14652777777777776\n",
      "Selected ('col1', 'col6') Size 32 Budget Used 0.1534722222222222\n",
      "Selected ('col5', 'col6') Size 10 Budget Used 0.16041666666666665\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 36.54767456482661\n",
      "Selected ('col1', 'col3') Size 240 Budget Used 0.18819444444444444\n",
      "Selected ('col1', 'col4') Size 96 Budget Used 0.2159722222222222\n",
      "Selected ('col0', 'col4') Size 54 Budget Used 0.24374999999999997\n",
      "Selected ('col2', 'col3') Size 105 Budget Used 0.2715277777777777\n",
      "Selected ('col4', 'col5') Size 30 Budget Used 0.2993055555555555\n",
      "Selected ('col0', 'col8') Size 18 Budget Used 0.3270833333333332\n",
      "Selected ('col1', 'col6') Size 32 Budget Used 0.354861111111111\n",
      "Selected ('col2', 'col5') Size 35 Budget Used 0.38263888888888875\n",
      "Selected ('col1',) Size 16 Budget Used 0.4104166666666665\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 18.273837282413304\n",
      "Selected ('col0', 'col3') Size 135 Budget Used 0.5215277777777776\n",
      "Selected ('col5', 'col7') Size 210 Budget Used 0.6326388888888888\n",
      "Selected ('col1', 'col4') Size 96 Budget Used 0.7437499999999998\n",
      "Selected ('col3', 'col4') Size 90 Budget Used 0.8548611111111108\n",
      "Selected ('col1', 'col3') Size 240 Budget Used 1.0\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 7.994413468365295\n",
      "Estimating marginals\n",
      "elapsed time= 360.200\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "aim = AIMSynthesizer(epsilon=1.0, delta=1e-9, verbose=True)\n",
    "aim.fit(adult_df_train, categorical_columns=categorical_columns)\n",
    "print(f'elapsed time= {time.time() - t0:.3f}')\n",
    "aim_adult_df = aim.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " GSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privacy budgets: Second moments = 0.014973\n",
      "Cond.Marginal= ['workclass', 'education'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['workclass', 'marital-status'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['workclass', 'occupation'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['workclass', 'relationship'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['workclass', 'race'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['workclass', 'sex'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['workclass', 'native-country'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['workclass', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['education', 'marital-status'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['education', 'occupation'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['education', 'relationship'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['education', 'race'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['education', 'sex'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['education', 'native-country'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['education', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['marital-status', 'occupation'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['marital-status', 'relationship'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['marital-status', 'race'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['marital-status', 'sex'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['marital-status', 'native-country'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['marital-status', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['occupation', 'relationship'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['occupation', 'race'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['occupation', 'sex'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['occupation', 'native-country'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['occupation', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['relationship', 'race'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['relationship', 'sex'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['relationship', 'native-country'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['relationship', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['race', 'sex'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['race', 'native-country'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['race', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['sex', 'native-country'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['sex', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "Cond.Marginal= ['native-country', 'earning-class'] . Sigma=0.0027. Top.Level=1. Max.Size=None\n",
      "\tTotal size=4186\n",
      "Statistics size = 4186\n",
      "Gen=      5000: fitness=3.88256401. Strategy wins:  [0.288 0.098 0.421 0.193] time=18.43(s)\n",
      "Gen=     10000: fitness=1.63625713. Strategy wins:  [0.137 0.24  0.538 0.086] time=28.23(s)\n",
      "Gen=     15000: fitness=0.56382656. Strategy wins:  [0.098 0.086 0.514 0.302] time=37.76(s)\n",
      "Gen=     20000: fitness=0.0734815443. Strategy wins:  [0.171 0.145 0.464 0.221] time=46.64(s)\n",
      "Gen=     25000: fitness=0.0137806233. Strategy wins:  [0.437 0.26  0.107 0.197] time=55.79(s)\n",
      "Gen=     30000: fitness=0.0108169563. Strategy wins:  [0.266 0.223 0.139 0.372] time=65.50(s)\n",
      "Gen=     35000: fitness=0.0103832421. Strategy wins:  [0.168 0.235 0.179 0.417] time=74.93(s)\n",
      "Gen=     40000: fitness=0.0102522209. Strategy wins:  [0.17  0.176 0.367 0.287] time=84.51(s)\n",
      "Gen=     45000: fitness=0.0101700514. Strategy wins:  [0.322 0.165 0.246 0.266] time=93.78(s)\n",
      "Gen=     50000: fitness=0.0101259389. Strategy wins:  [0.175 0.292 0.169 0.364] time=102.95(s)\n",
      "Gen=     55000: fitness=0.0100910233. Strategy wins:  [0.293 0.198 0.35  0.158] time=113.38(s)\n",
      "Gen=     60000: fitness=0.0100652207. Strategy wins:  [0.212 0.132 0.396 0.26 ] time=122.98(s)\n",
      "Gen=     65000: fitness=0.0100493422. Strategy wins:  [0.366 0.152 0.216 0.266] time=132.19(s)\n",
      "Gen=     70000: fitness=0.0100346154. Strategy wins:  [0.247 0.248 0.244 0.261] time=141.57(s)\n",
      "Gen=     75000: fitness=0.0100207477. Strategy wins:  [0.068 0.278 0.383 0.271] time=151.65(s)\n",
      "Gen=     80000: fitness=0.0100112948. Strategy wins:  [0.02  0.562 0.243 0.175] time=161.38(s)\n",
      "Gen=     85000: fitness=0.0100075904. Strategy wins:  [0.006 0.705 0.279 0.01 ] time=173.52(s)\n",
      "Gen=     90000: fitness=0.0100044387. Strategy wins:  [0.002 0.959 0.039 0.   ] time=187.89(s)\n",
      "Gen=     95000: fitness=0.00999953425. Strategy wins:  [0.004 0.565 0.43  0.   ] time=202.57(s)\n",
      "Gen=    100000: fitness=0.00999793999. Strategy wins:  [0.    0.814 0.186 0.   ] time=214.74(s)\n",
      "Gen=    105000: fitness=0.0099947214. Strategy wins:  [0.001 0.629 0.371 0.   ] time=229.55(s)\n",
      "Gen=    110000: fitness=0.00999186076. Strategy wins:  [0.    0.669 0.331 0.   ] time=241.65(s)\n",
      "Gen=    115000: fitness=0.00998913589. Strategy wins:  [0.    0.937 0.063 0.   ] time=253.59(s)\n",
      "Gen=    120000: fitness=0.00998729747. Strategy wins:  [0.003 0.995 0.002 0.   ] time=268.18(s)\n",
      "Gen=    125000: fitness=0.00998531184. Strategy wins:  [0. 1. 0. 0.] time=282.96(s)\n",
      "Gen=    130000: fitness=0.00998327858. Strategy wins:  [0. 1. 0. 0.] time=297.88(s)\n",
      "Gen=    135000: fitness=0.00998119965. Strategy wins:  [0. 1. 0. 0.] time=312.85(s)\n",
      "Gen=    140000: fitness=0.0099803874. Strategy wins:  [0.001 0.999 0.    0.   ] time=327.88(s)\n",
      "\t\t ### Stop early at 140000 ###\n",
      "elapsed time= 335.535\n"
     ]
    }
   ],
   "source": [
    "# Run GSD\n",
    "\n",
    "t0 = time.time()\n",
    "gsd = GSDSynthesizer(epsilon=1.0, delta=1e-9, verbose=True)\n",
    "gsd.fit(adult_df_train, N_prime=5000, categorical_columns=categorical_columns)\n",
    "print(f'elapsed time= {time.time() - t0:.3f}')\n",
    "gsd_adult_df = gsd.sample()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this section, we construct a toy dataset with one categorical feature with cardinality 10000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cond.Marginal= [0] . Sigma=0.0000. Top.Level=1. Max.Size=None\n",
      "\tTotal size=6328\n",
      "Statistics size = 6328\n",
      "Gen=      1000: fitness=0.00893408622. Strategy wins:  [0.426 0.574] time=3.94 (s)\n",
      "Gen=      2000: fitness=0.000379580813. Strategy wins:  [1. 0.] time=5.20 (s)\n",
      "Gen=      3000: fitness=0.000377139804. Strategy wins:  [1. 0.] time=6.30 (s)\n",
      "Gen=      4000: fitness=0.000376495727. Strategy wins:  [1. 0.] time=7.43 (s)\n",
      "Gen=      5000: fitness=0.000376359241. Strategy wins:  [1. 0.] time=8.52 (s)\n",
      "Gen=      6000: fitness=0.000376350038. Strategy wins:  [1. 0.] time=9.60 (s)\n",
      "\t\t ### Stop early at 6000 ###\n",
      "elapsed time = 15.244399785995483\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epsilon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melapsed time = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(synth\u001b[38;5;241m.\u001b[39mstat_fn(synth\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto_numpy()) \u001b[38;5;241m-\u001b[39m synth\u001b[38;5;241m.\u001b[39mstat_fn(synth\u001b[38;5;241m.\u001b[39msync_data\u001b[38;5;241m.\u001b[39mto_numpy()))\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatistical error: max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Avg=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilon' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "d = 1\n",
    "ordinal_columns = 'categorical_1'\n",
    "cat_cardinality = 10000\n",
    "N = 2 * cat_cardinality\n",
    "\n",
    "data_np = rng.integers(0, cat_cardinality, (N, d ))\n",
    "data_np[:cat_cardinality, 0] = 0\n",
    "data_np = data_np.astype(str)\n",
    "data_df = pd.DataFrame(data_np, columns=[ordinal_columns])\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "synth = GSDSynthesizer(3000.0, 1e-5, verbose=True)\n",
    "# Since we are passing the data bounds, we do not need to provide privacy budget for preprocessing.\n",
    "synth.fit(data_np, N_prime=1000, genetic_operators=['mutate', 'cross'])\n",
    "print(f'elapsed time = {time.time() - t0}')\n",
    "\n",
    "error = np.abs(synth.stat_fn(synth.data.to_numpy()) - synth.stat_fn(synth.sync_data.to_numpy()))\n",
    "print(f'epsilon={epsilon}', f'Statistical error: max={error.max():.4f}, Avg={error.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to observe slow converge behavior without the cross genetic operator.\n",
    "t0 = time.time()\n",
    "synth = GSDSynthesizer(3000.0, 1e-5, verbose=True)\n",
    "# Since we are passing the data bounds, we do not need to provide privacy budget for preprocessing.\n",
    "synth.fit(data_df, N_prime=1000, genetic_operators=['mutate'])\n",
    "print(f'elapsed time = {time.time() - t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "d = 1\n",
    "columns = ['categorical_1', 'categorical_2']\n",
    "cat_cardinality = 11\n",
    "N = 200\n",
    "values = rng.integers(1, cat_cardinality, (N, d ))\n",
    "values[:190] = 0\n",
    "data_np = np.column_stack((values, values)).astype(str)\n",
    "correlated_cat_df = pd.DataFrame(data_np, columns=columns)\n",
    "\n",
    "t0 = time.time()\n",
    "synth = GSDSynthesizer(10000000.0, 1e-5, verbose=True)\n",
    "# Since we are passing the data bounds, we do not need to provide privacy budget for preprocessing.\n",
    "synth.fit(correlated_cat_df, N_prime=200, genetic_operators=['mutate', 'cross', 'swap'])\n",
    "print(f'elapsed time = {time.time() - t0}')\n",
    "\n",
    "error = np.abs(synth.stat_fn(synth.data.to_numpy()) - synth.stat_fn(synth.sync_data.to_numpy()))\n",
    "print(f'epsilon={epsilon}', f'Statistical error: max={error.max():.4f}, Avg={error.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to check slow convergence without the cross operator\n",
    "t0 = time.time()\n",
    "synth = GSDSynthesizer(10000000.0, 1e-5, verbose=True)\n",
    "# Since we are passing the data bounds, we do not need to provide privacy budget for preprocessing.\n",
    "synth.fit(correlated_cat_df, N_prime=200, genetic_operators=['mutate', 'cross'])\n",
    "print(f'elapsed time = {time.time() - t0}')\n",
    "\n",
    "error = np.abs(synth.stat_fn(synth.data.to_numpy()) - synth.stat_fn(synth.sync_data.to_numpy()))\n",
    "print(f'epsilon={epsilon}', f'Statistical error: max={error.max():.4f}, Avg={error.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3-d correlated categorical data.\n",
    "\"\"\"\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "d = 1\n",
    "columns = ['categorical_1', 'categorical_2', 'categorical_3']\n",
    "cat_cardinality = 11\n",
    "N = 200\n",
    "values = rng.integers(1, cat_cardinality, (N, d ))\n",
    "values[:190] = 0\n",
    "data_np = np.column_stack((values, values, values)).astype(str)\n",
    "correlated_cat_df = pd.DataFrame(data_np, columns=columns)\n",
    "\n",
    "t0 = time.time()\n",
    "synth = GSDSynthesizer(10000000.0, 1e-5, verbose=True)\n",
    "# Since we are passing the data bounds, we do not need to provide privacy budget for preprocessing.\n",
    "synth.fit(correlated_cat_df, N_prime=200, genetic_operators=['mutate', 'cross'])\n",
    "print(f'elapsed time = {time.time() - t0}')\n",
    "\n",
    "error = np.abs(synth.stat_fn(synth.data.to_numpy()) - synth.stat_fn(synth.sync_data.to_numpy()))\n",
    "print(f'epsilon={epsilon}', f'Statistical error: max={error.max():.4f}, Avg={error.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the train on synthetic, test on real approach:\n",
    "\n",
    "- Given two sets of real data: A train set and ad test set.\n",
    "- Generate synthetic data using the train set.\n",
    "- Then we train a ML model using the synthetic data.\n",
    "- Finally, we validate the model's performance using the holdout real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "categorical_features = categorical_columns.copy()\n",
    "label = 'earning-class'\n",
    "categorical_features.remove(label)\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", GradientBoostingClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on real:\n",
      "Accuracy=0.8349\n",
      "F1-score = 0.7528\n"
     ]
    }
   ],
   "source": [
    "X_real = adult_df_train[categorical_features]\n",
    "y_real = adult_df_train[label]\n",
    "\n",
    "\n",
    "X_test = adult_df_test[categorical_features]\n",
    "y_test = adult_df_test[label]\n",
    "\n",
    "clf.fit(X_real, y_real)\n",
    "\n",
    "print(f'Train on real:')\n",
    "print(f\"Accuracy={clf.score(X_test, y_test):.4f}\")\n",
    "print(f\"F1-score = {f1_score(y_test, clf.predict(X_test), average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on AIM synthetic data:\n",
      "Accuracy = 0.8303\n",
      "F1-score = 0.7461\n"
     ]
    }
   ],
   "source": [
    "X_aim = aim_adult_df[categorical_features]\n",
    "y_aim = aim_adult_df[label]\n",
    "\n",
    "clf.fit(X_aim, y_aim)\n",
    "\n",
    "\n",
    "print(f'Train on AIM synthetic data:')\n",
    "print(f\"Accuracy = {clf.score(X_test, y_test):.4f}\")\n",
    "print(f\"F1-score = {f1_score(y_test, clf.predict(X_test), average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GSD synthetic data:\n",
      "Accuracy = 0.8349\n",
      "F1-score = 0.7494\n"
     ]
    }
   ],
   "source": [
    "X_gsd = gsd_adult_df[categorical_features]\n",
    "y_gsd = gsd_adult_df[label]\n",
    "\n",
    "clf.fit(X_gsd, y_gsd)\n",
    "\n",
    "print(f'Train on GSD synthetic data:')\n",
    "print(f\"Accuracy = {clf.score(X_test, y_test):.4f}\")\n",
    "print(f\"F1-score = {f1_score(y_test, clf.predict(X_test), average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "collect = []\n",
    "\n",
    "mean = [0.5, 0.5]\n",
    "cov = [[.01, 0.0099], \n",
    "       [0.0099, .01]]  # diagonal covariance\n",
    "\n",
    "N = 200\n",
    "\n",
    "x, y = np.random.multivariate_normal(mean, cov, N).T\n",
    "values_cont = np.column_stack((x, y))\n",
    "values_cont[100:300, 0] = 0.03\n",
    "values_cont[100:300, 1] = 0.93\n",
    "\n",
    "cont_cols = ['c1', 'c2']\n",
    "data_cont_df = pd.DataFrame(values_cont, columns=cont_cols)\n",
    "meta_data = {'c1': {'type': 'float', 'lower': 0, 'upper': 1}, 'c2': {'type': 'float', 'lower': 0, 'upper': 1}}\n",
    "\n",
    "plot_data = data_cont_df.copy()\n",
    "plot_data.loc[:, 'Type'] = 'Original'\n",
    "collect.append(plot_data)\n",
    "\n",
    "\n",
    "for epsilon in [1000, 100]:\n",
    "    print(f'epsilon={epsilon}')\n",
    "    synth = GSDSynthesizer(float(epsilon), 1e-5, tree_height=12, verbose=True)\n",
    "    synth.fit(data_cont_df, meta_data=meta_data, \n",
    "              genetic_operators=['mutate', 'continuous'])\n",
    "    error = np.abs(synth.stat_fn(synth.data.to_numpy()) - synth.stat_fn(synth.sync_data.to_numpy()))\n",
    "    print(f'epsilon={epsilon}', f'Statistical error: max={error.max():.4f}, Avg={error.mean():.4f}')\n",
    "\n",
    "    sync_df = synth.sample()\n",
    "    sync_df.loc[:, 'Type'] = f'eps={epsilon}'\n",
    "    collect.append(sync_df)\n",
    "\n",
    "    synth = GSDSynthesizer(float(epsilon), 1e-5, tree_height=12, verbose=True)\n",
    "    # Since we are passing the data bounds, we do not need to provide privacy budget for preprocessing.\n",
    "    synth.fit(data_cont_df, meta_data=meta_data, \n",
    "              genetic_operators=['mutate', 'continuous', 'swap'])\n",
    "    error = np.abs(synth.stat_fn(synth.data.to_numpy()) - synth.stat_fn(synth.sync_data.to_numpy()))\n",
    "    print(f'epsilon={epsilon}', f'Statistical error: max={error.max():.4f}, Avg={error.mean():.4f}')\n",
    "    sync_df = synth.sample()\n",
    "    sync_df.loc[:, 'Type'] = f'eps={epsilon}/swap'\n",
    "    collect.append(sync_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(collect)\n",
    "g= sns.FacetGrid(data=all_data, col='Type', sharey=True,sharex=True)\n",
    "g.map(sns.scatterplot, \"c1\", \"c2\", alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
